{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Magnification loop example\n",
    "\n",
    "Recent papers have shown that training models using multiple magnifications could greatly improve model performance, i.e. mixing patches extracted with both 20x and 10x magnification instead of only using one or the other.\n",
    "\n",
    "This notebook presents how to use the `WSIDataloader` class to easily mix magnifications in patches extracted in the Dataset.\n",
    "\n",
    "To define the `patch_generator` function, we chose to use the [`TIAToolbox`](https://github.com/TissueImageAnalytics/tiatoolbox) library, but any other similar implementation using ``openslide`` as a backend should work.\n",
    "\n",
    "This notebook was executed on a CentOS machine with 32GB of RAM, a Core i9 13900 and an RTX4090. We strongly recommend storing the WSIs on an SSD for fast random access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below cell, we create two things:\n",
    "1. The `CustomSlidingWindowPatchExtractor` class: inherits from TIAToolbox's `SlidingWindowPatchExtractor`, only adding a definition for the `__len__(self)` method, to use it as an iterator.\n",
    "\n",
    "2. The `get_patches(wsi_path)` function: This functions takes the path to a WSI as input and outputs an iterator over all the WSI patches as defined by the given parameters. The `mask` used here is the simplest `TIAToolbox` Otsu mask implementation. Note that the resolution parameter is not fixed, we define it as a random sample between 3 options: 20x, 10x and 5x.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from tiatoolbox.tools.patchextraction import SlidingWindowPatchExtractor\n",
    "from tiatoolbox.wsicore.wsireader import WSIReader\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torch.nn import Sequential\n",
    "from tqdm import tqdm\n",
    "\n",
    "from wsiloader import WSIDataloader\n",
    "\n",
    "\n",
    "class CustiomSlidingWindowPatchExtractor(SlidingWindowPatchExtractor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.locations_df.shape[0] if self.locations_df is not None else 0\n",
    "\n",
    "\n",
    "def get_patches(wsi_path: str):\n",
    "    wsi = WSIReader.open(input_img=wsi_path)\n",
    "\n",
    "    # This mask can and should be adaptated to your data\n",
    "    # Here we use a simple Otsu\n",
    "    mask = wsi.tissue_mask(resolution=1.25, units=\"power\")\n",
    "\n",
    "    # All of these parameters can and should be adapted to your specific needs\n",
    "    patch_size = 224  # Square patch of size 224x224\n",
    "    resolution = np.random.choice([20, 10, 5]) # Randomly chose a magnification objective between 20x, 10x and 5x\n",
    "    overlap    = 0.0  # No overlap between extracted patches\n",
    "\n",
    "    patches = CustiomSlidingWindowPatchExtractor(\n",
    "        input_img=wsi,\n",
    "        patch_size=(patch_size,)*2,\n",
    "        stride=(patch_size - int(overlap*patch_size),)*2,\n",
    "        resolution=resolution,\n",
    "        units=\"power\",\n",
    "        input_mask=mask,\n",
    "        min_mask_ratio=0.3,\n",
    "        within_bound=True,\n",
    "    )\n",
    "\n",
    "    return patches\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we create a `WSIDataloader` instance. For this example we used a subset of 27 WSIs (39GB total) from the normal train set of the [Camelyon16](https://camelyon16.grand-challenge.org/Data/) dataset. We also define a set of strong augmentations typically seen in various contrastive learning frameworks. Note that instead of the usual `transforms.Compose` we use `torch.nn.Sequential` to enable the tranforms execution on the available cuda device. The `WSIDataloader` class will then load the data using multiple `torch.utils.data.Dataloader` CPU workers and will then apply the transforms on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building WSI index:   0%|                                                                        | 0/26 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building WSI index: 100%|██████████████████████████████| 26/26 [00:25<00:00,  1.03it/s, [8612 > 72,940 indexed patches]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# List WSI samples from the normal train set of the Camelyon16 dataset\n",
    "wsi_paths = list(Path(\"/home/travail/data/camelyon\").glob(\"*.tif\"))\n",
    "\n",
    "augmentations = Sequential(\n",
    "    transforms.RandomResizedCrop(224, scale=(0.2, 1.), antialias=True),\n",
    "    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(3, [.1, 2.])], p=0.5),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.Normalize(mean=[0.6684, 0.5115, 0.6791],\n",
    "                         std=[0.2521, 0.2875, 0.2100])\n",
    ")\n",
    "augmentations.to(\"cuda\")\n",
    "\n",
    "# Pytorch Dataloader Parameters:\n",
    "dataloader_params = dict(\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "loader = WSIDataloader(\n",
    "    wsi_paths=wsi_paths,\n",
    "    patch_generator=get_patches,\n",
    "    transforms=augmentations,\n",
    "    transforms_device=\"cuda\",\n",
    "    **dataloader_params,\n",
    ")\n",
    "print(f\"Number of batches: {len(loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the above number of indexed patches: 72 940.\n",
    "These patches are a mix of patches extracted at 20x, 10x and 5x magnification objectives.\n",
    "\n",
    "These resolutions are for now tied to specific WSIs, but we can reset the patch index using the `WSIDataloader.reset_index()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building WSI index: 100%|██████████████████████████████| 26/26 [00:31<00:00,  1.22s/it, [2055 > 47,325 indexed patches]]\n"
     ]
    }
   ],
   "source": [
    "loader.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, 47 325 patches have been indexed, meaning that different magnification objectives were set for each WSI. In a training loop, the `reset_index()` method could called after every epoch, to resample patches and apply different magnification objectives. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "placenta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
